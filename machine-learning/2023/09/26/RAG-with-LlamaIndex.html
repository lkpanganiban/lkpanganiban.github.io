<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1 maximum-scale=1; minimum-scale=1; user-scalable=no;"> <meta content="Over an afternoon's hacking, I've managed to setup a Large Language Model that implements a Retrieval Augmented Generation or RAG architecture locally. This enables LLMs to work on your own and up-to-date data without the need of retraining. In this post, I will be going through my initial implementation of the retrieval augmented generation or RAG systems." name="description"> <meta name="keywords" content="machine-learning,mlops"> <meta name="author" content="lkpanganiban"> <meta name="baseurl" content=""> <title> lkpanganiban|RAG with Llama Index </title> <!-- favicon --> <link rel="shortcut icon" href="/static/assets/img/favicon.png"> <!-- Main CSS --> <link href="/static/assets/app-20210315.min.css" rel="stylesheet"> <link href="/static/css/custom.css" rel="stylesheet"> <!-- Main Scripts --> <script src="/static/assets/app-20210315.min.js"></script> <script src="/static/assets/blog-20210315.min.js"></script> <!-- Google AdSense --> <!-- <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> <script> (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: "ca-pub-6196184620210315", enable_page_level_ads: true }); </script> --> </head> <body id="page-top" class="landing-page"> <div class="search-tool" style="position: fixed; top: 0px ; bottom: 0px; left: 0px; right: 0px; opacity: 0.95; background-color: #111111; z-index: 9999; display: none;"> <input type="text" class="form-control search-content" id="search-content" style="position: fixed; top: 60px" placeholder="Search Blog"> <div style="position: fixed; top: 16px; right: 16px; z-index: 9999;"> <img src="/static/assets/img/search/cb-close.png" id="close-btn"/> </div> </div> <div style="position: fixed; right: 16px; bottom: 20px; z-index: 9999;"> <img src="/static/assets/img/search/cb-search.png" id="search-btn" title="Double click Ctrl"/> </div> <div class="navbar-wrapper"> <nav class="navbar navbar-default navbar-fixed-top" role="navigation"> <div class="container"> <div class="navbar-header page-scroll"> <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/">lkpanganiban</a> </div> <div id="navbar" class="navbar-collapse collapse"> <ul class="nav navbar-nav navbar-right"> <li><a class="page-scroll" href="blog/"></a></li> <li> <a class="page-scroll" href="/blog/">Blog</a></li> <li> <a class="page-scroll" href="/python/">Python</a></li> <li> <a class="page-scroll" href="/machine-learning/">Machine Learning</a></li> <li> <a class="page-scroll" href="/database/">Database</a></li> <li> <a class="page-scroll" href="/devops/">Devops</a></li> </ul> </div> </div> </nav> </div> <div class="container-fluid"> <div id="inSlider" class="carousel slide" data-ride="carousel"> <!-- Indicators --> <ol class="carousel-indicators"> <li data-target="#inSlider" data-slide-to="0" class="active"></li> <li data-target="#inSlider" data-slide-to="1"></li> </ol> <!-- Wrapper for slides --> <div class="carousel-inner"> <div class="item active"> <div class="carousel-caption"></div> <img src="/static/assets/img/landing/header_one.webp" alt="first banner image" style="width:100%;"> </div> <div class="item"> <div class="carousel-caption"></div> <img src="/static/assets/img/landing/header_two.jpg" alt="second banner image" style="width:100%;"> </div> </div> <!-- Left and right controls --> <a class="left carousel-control" href="#inSlider" data-slide="prev"> <span class="glyphicon glyphicon-chevron-left"></span> <span class="sr-only">Previous</span> </a> <a class="right carousel-control" href="#inSlider" data-slide="next"> <span class="glyphicon glyphicon-chevron-right"></span> <span class="sr-only">Next</span> </a> </div> </div> <div class="wrapper wrapper-content animated fadeInRight article"> <div class="row"> <div class="col-lg-10 col-lg-offset-1"> <div class="ibox"> <div class="ibox-content"> <div class="pull-right"> <a class="btn btn-white btn-xs" href="/machine-learning">Machine-learning</a> </div> <div class="text-center article-title"> <span class="text-muted"><i class="fa fa-clock-o"></i> 26 Sep 2023</span> <h1> RAG with Llama Index </h1> </div> <p>Over an afternoon’s hacking, I’ve managed to setup a Large Language Model that implements a Retrieval Augmented Generation or RAG architecture locally. This enables LLMs to work on your own and up-to-date data without the need of retraining. In this post, I will be going through my initial implementation of the retrieval augmented generation or RAG systems.</p> <h1 id="objective">Objective</h1> <p>The objective of this exercise is to setup and run a RAG implementation using LLMs with generally available hardware locally (no dedicated GPUs and no cloud!). <em>I want to remove the barrier of expensive hardware and giving cloud providers more money and my information.</em></p> <h1 id="what-is-rag">What is RAG?</h1> <p>Retrieval Augmented Generation or RAG for short allows the LLM to work with new data without the need of retraining. RAG has the following components:</p> <ul> <li>Vector Database (Elasticsearch, Pinecone, PostgreSQL Vector Plugin)</li> <li>Embedding Model (maybe different from the actual LLM)</li> <li>Large Language Model</li> </ul> <h2 id="general-flow">General Flow</h2> <p align="center"> <img src="/static/assets/img/blog/20230926/rag_diagram.png" alt="rag-diagram" /> </p> <ol> <li>Data Ingestion: Using an embedding model, the data will be ingested (texts in this case) into this database by converting them into vectors. As an example, the following sentece “A TIFF file is a raster format.” will be converted into <code class="language-plaintext highlighter-rouge">[0.211, 0.113, 0.44112, -0.1294]</code> vector and this vector will be stored into the vector database.</li> <li>Search and Retrieval: When a question comes in “Are TIFF files raster data?” the question will also be converted into a vector and the vector database will implement a clustering algorithm like approximate nearest neighbor, K-nearest neighbor, inverted index, etc.</li> <li>Summarization and Response: The returned results will be passed to the LLM and execute summarization task and thus generate the final output.</li> </ol> <h1 id="the-setup">The Setup</h1> <h2 id="hardware">Hardware</h2> <p><em>What I want to achieve in this experiment is to run the process without very expensive components like GPUs.</em></p> <ul> <li>CPU: Ryzen 4500u</li> <li>RAM: 16GB</li> <li>GPU: Integrated GPU (6 graphics cores)</li> <li>Operating: Ubuntu 22.04 running in WSL</li> <li>Swap: 0</li> </ul> <h2 id="models-and-data">Models and Data:</h2> <p><em>The models came from huggingface.co.</em></p> <ul> <li>Large Language Model: llama-2-7b-chat.Q4_0.gguf</li> <li>Embedding Model: sentence-transformers/all-mpnet-base-v2</li> <li>Data: QGIS 3.22 User Manual (1393 pages with files size of 50.9mb)</li> </ul> <h2 id="libraries">Libraries:</h2> <p><em>You may need to extra setup to configure LlamaCPP properly for your own hardware.</em></p> <ul> <li>Langchain</li> <li>LlamaCPP</li> <li>Llama Index</li> <li>PyPDF (to extract texts from PDFs)</li> </ul> <h1 id="the-experiment">The Experiment</h1> <p>I have created a notebook to go through the actual code <a href="https://github.com/lkpanganiban/llama-index-experiment">here.</a> In this post, I will just go through my observations of the processing flow.</p> <ol> <li>Data ingestion will take the longest part of the entire process and maybe resource intensive depending on the volume in terms of pages and files. This step will do the extraction from the source file, conversion of text into vector data, and store into the vector DB. In this exercise, it took about <code class="language-plaintext highlighter-rouge">12-15 mins</code> to ingest a 1,393 page document and consumes about 6 GB of RAM for a 6 core machine. <em>If your machine has more CPU cores or if the system has GPU this will be faster to run.</em> <p align="center"> <img src="/static/assets/img/blog/20230926/ingestion-usage.png" alt="ingestion-usage" /> </p> </li> <li> <p>Depending on the model parameters like chunk_overlap, temperature, context_window, etc., the results may vary. There is a need to explore the sensitivity of each parameter in terms of generation performance.</p> </li> <li>The inferencing section, executed about <code class="language-plaintext highlighter-rouge">2-3 mins</code>. <p align="center"> <img src="/static/assets/img/blog/20230926/llama-result.png" alt="llama-result" /> </p> </li> <li>The generated results have some errors and redundant steps.</li> </ol> <h2 id="experiment-notes">Experiment Notes</h2> <ul> <li>Returned results from the Vector DB (what are the paragraphs or sentences - do they make sense?)</li> <li>Prompt setup to implement summarization (from the returned results, what is the prompt used?)</li> <li>Model itself (since we are using a quantized model which uses int4 there maybe a loss of precision which affected the generated results. There is also how the model was trained where it will affect the quality of the results.)</li> </ul> <h1 id="next-steps">Next Steps</h1> <ol> <li>Try out other models more specialized to my use case which is more of an instruction type instead of chat.</li> <li>Play around the with parameters like chunk_overlap, temperature, context_window, etc.</li> <li>Extend the RAG system to not only focus in a vector setup but also with a mixture of text data (Bag of Words).</li> <li>Incorporate evaluation, where given multiple models, it will evaluate which output will be used as the main result or mix the outputs of these models. <em>This is like ensemble learning where you a mixed of experts/models to generate a result.</em></li> <li>Create agents on-top of RAGs, where it will control a software using the outputs of the model. <em>Given the outputs above, control QGIS to do the steps stated by LLM.</em> I see the use case of e.g. I want to generate a flood model using the data in folder A where the model will follow papers X, Y, Z.</li> </ol> <h1 id="notes-takeaways-opinions">Notes, Takeaways, Opinions</h1> <ul> <li>The outputs generated by the LLM may not be 100% accurate but for me this is enough to get me started or point me to the right direction. <em>You will still need to evaluate the outputs if it makes sense.</em></li> <li>The barrier of going into the LLM space is still there but it is going down exponentially with better architecture, tooling, and ecosystem.</li> <li>For me, the better use for LLMs will be in the RAG type applications where every individual, company, or organization have their “secret” sauce. Hence, running it locally is of great value instead of offloading the system to an external provider.</li> </ul> <hr> <div class="row"> <div class="col-md-6"> <h5 style="display: inline;">Tags:</h5> <button class="btn btn-white btn-xs" type="button">machine-learning</button> </div> <div class="col-md-6"> <div class="small text-right"> <div> </div> </div> </div> </div> <br> <div class="row"> <div class="col-lg-12"> <!-- donate --> <br> <!-- share --> <div class="a2a_kit a2a_kit_size_32 a2a_default_style"> <a class="a2a_dd" href="https://www.addtoany.com/share"></a> <a class="a2a_button_facebook"></a> <a class="a2a_button_twitter"></a> <a class="a2a_button_google_plus"></a> <a class="a2a_button_linkedin"></a> <a class="a2a_button_email"></a> <a class="a2a_button_wechat"></a> <a class="a2a_button_sina_weibo"></a> <a class="a2a_button_pocket"></a> </div> <script> var a2a_config = a2a_config || {}; a2a_config.color_main = "D7E5ED"; a2a_config.color_border = "AECADB"; a2a_config.color_link_text = "333333"; a2a_config.color_link_text_hover = "333333"; </script> <script async src="https://static.addtoany.com/menu/page.js"></script> <br> <!-- comment --> </div> </div> </div> </div> </div> </div> </div> <!-- Google analytics --> <!-- GrowingIO --> </body> </html>
